# Applying quantum algorithms to robotics tasks offers a more intuitive approach to learning and understanding their design.

Abstract:
Robotics is often learners' first introduction to computer science concepts due to its hands-on nature. Quantum robotics is a quickly evolving field which holds similar promise. This poster introduces a novel curriculum for teaching about the core concepts of quantum computing through quantum agent models implemented for managing robotsâ€™ decision processes. Specifically, it delves into the use of a variety of quantum algorithms, such as quantum random walks, to be used in tandem with classical reinforcement learning algorithms in order to enable robots to improve their performance on specified design goals. Such an approach allows learners to visually understand the impacts that quantum algorithms can have, a skill directly correlated to higher learner retention rates.


1. Learning Grover's Algorithm using path planning tasks: This approach utilizes amplitude amplification to find the best path through a predetermined environment.

The OpenAI Gymnasium library provides benchmarking tools to simulate robot path planning tasks. The FrozenLake environments has 16 states (entries in the grid) and 4 actions (directions the robot can move in). This algorithm achieves consistent accuracy after 3000 epochs of training. The training process for the Grover-inspired path planning algorithm results in 16 trained action circuits, each of which amplify the most likely action to take when on the specified square. The number of repetitions of the amplification operations is determined by the rewards received for each state. Finally, the output of the action circuit directly translates to the provided actions.

2. Learning Quantum Key Distribution using secure communications tasks: The BB-84 protocol can be visualized in terms of multiple robots attempting secure communications.

The optical experiments for this study are conducted using VQOL (a JavaScript library for photonics experiments). The lasers in both experiments encode a bit with a value of 0 in the diagonal basis, representing a robot sending an outgoing signal with the value of 0. The first experiment demonstrates how a set of two robots can detect if communications have been intercepted. The laser to the far left represents the robot sending the outgoing signal. The intercepting robot is represented by the polarizing beam splitter in the rectilinear basis and photon detector #3. Because the act of eavesdropping changes the polarization of the bit, the listening robot measures the wrong value of the bit, which can be detected during classical communications between the two intended robots after the signals have been transmitted. The experiment results in successfully transmitting and receiving a bit with a value of 0 only when the following two conditions are met: (1) There are no eavesdroppers present AND (2) The robot sending the signal and the robot receiving the signal have chosen the same basis (in this case, the diagonal basis).

3. Learning Quantum Random Walks using path planning tasks: Quantum walks are the key to traversing a network of states and actions in the larger quantum projective simulation algorithm.

The FrozenLake benchmark is used to test the quantum projective simulation algorithm. The agent (robot, in this context) interacts with the environment (frozen lake, in this context) through states and actions. The current state is the input, while the next action is the output of the algorithm. The key to projective simulation lies in the ECM, a network of nodes representing each experience the robot remembers. These nodes can be either states (s1, s2, etc.) or actions (a1, a2, a3, or a4). In order for the agent to choose the correct action according to past experiences from the network, the network is modeled as a transition matrix for a Markov chain and random walks are computed, amplifying the actions of the highest likelihood that are rewarded the most. The matrix contains the probabilities of transitioning from one node to the other in the directed, weighted network called the ECM.

4. Learning Quantum Positional Verification (QPV) using navigation tasks: QPV is a protocol that allows a robot to be located, which is essential for larger navigation-related tasks.

QPV requires two external vehicles (the verifiers) to locate the robot in question. Each verifier chooses a value that satisfies a condition, xy = pi/2. This condition ensures the entanglement phase of QPV can be carried out to its full capacity. Both x and y are sent to the prover via a classical channel. The prover uses an entanglement source to prepare this state using the two values received from both Verifiers. The entanglement source communicates with the two verifiers by sending a pair of photons entangled in this state, both in the same polarization. Each of the verifiers has a polarizing beam splitter and set of photon detectors to perform a Bell test on this information sent from the prover. This procedure must be repeated at least twice in order for the verifiers to measure the signals in two bases (required by the Bell test).

5. Learning Quantum Neural Networks (QNNs) using trajectory planning tasks: QNNs are the fully Quantum Reinforcement Learning strategy that are implementable on NISQ devices.

The OpenAI Gymnasium benchmark used to test the QNN is CartPole. The inputs (states) provided are the cart position, cart velocity, pole position, and pole velocity. In order for the algorithm to successfully keep the pole on the cart upright, it can decide to move the cart left or right (as actions). The classical pre-processing step takes the dot product of the 4 inputs in a vector with a vector of 4 trainable parameters. The RX gates encode the modified weights created by the classical pre-processing step, after which the variational ansatz with trainable parameters transforms those values and circularly entangles them to create a level of interaction between the variables. This sequence of gates can be repeated to add layers to the QNN. The probability distribution created by the output of the circuit is then used to calculate the expected reward for each action. The expected rewards can then be used to make the final decision on what action to take. In the context of CartPole, this is simply whether to move left or right.

Citations:

